{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import time\n",
    "import pygame\n",
    "from sklearn.preprocessing import normalize\n",
    "import tensorflow as tf\n",
    "import tflearn\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"MsPacman-v0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Num actions: 9'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"Num actions: \" + str(env.action_space.n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Observation space: Box(210, 160, 3)'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"Observation space: \" + str(env.observation_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ejmejm/anaconda3/lib/python3.5/site-packages/tflearn/initializations.py:119: UniformUnitScaling.__init__ (from tensorflow.python.ops.init_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.initializers.variance_scaling instead with distribution=uniform to get equivalent behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ejmejm/anaconda3/lib/python3.5/site-packages/tensorflow/python/ops/gradients_impl.py:96: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    }
   ],
   "source": [
    "observation = tflearn.input_data(shape=[None, 210, 160, 3])\n",
    "net = tflearn.max_pool_2d(observation, 2)\n",
    "w1 = tflearn.conv_2d(observation, 32, 3, padding='valid', activation='relu')\n",
    "net = tflearn.max_pool_2d(w1, 2)\n",
    "net = tflearn.conv_2d(net, 32, 3, padding='valid', activation='relu')\n",
    "net = tflearn.max_pool_2d(net, 2)\n",
    "net = tflearn.conv_2d(net, 64, 3, padding='valid', activation='relu')\n",
    "net = tflearn.max_pool_2d(net, 2)\n",
    "net = tflearn.conv_2d(net, 64, 3, padding='valid', activation='relu')\n",
    "net = tflearn.max_pool_2d(net, 2)\n",
    "net = tflearn.conv_2d(net, 128, 3, padding='valid', activation='relu')\n",
    "net = tflearn.max_pool_2d(net, 2)\n",
    "net = tflearn.conv_2d(net, 128, 3, padding='valid', activation='relu')\n",
    "net = tflearn.fully_connected(net, 1028, activation='relu')\n",
    "out = tflearn.fully_connected(net, env.action_space.n, activation=\"softmax\")\n",
    "\n",
    "reward_holder = tf.placeholder(tf.float32, [None])\n",
    "action_holder = tf.placeholder(tf.int32, [None])\n",
    "\n",
    "responsible_outputs = tf.gather(tf.reshape(out, [-1]), tf.range(0, tf.shape(out)[0] * tf.shape(out)[1], env.action_space.n) + action_holder)\n",
    "\n",
    "loss = -tf.reduce_mean(tf.log(responsible_outputs + 0.0001) * reward_holder)\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=0.0005)\n",
    "update = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = 0.99\n",
    "\n",
    "def discount_reward(rewards):\n",
    "    running_reward = 0\n",
    "    result = np.zeros_like(rewards)\n",
    "    for i in reversed(range(len(rewards))):\n",
    "        result[i] = rewards[i] + gamma * running_reward\n",
    "        running_reward += rewards[i]\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pixelNorm(img):\n",
    "    return img / 255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reward: 296.5\n",
      "Reward: 382.5\n",
      "Reward: 326.0\n",
      "Reward: 270.0\n",
      "Reward: 317.5\n",
      "Reward: 281.5\n",
      "Reward: 297.0\n",
      "Reward: 189.5\n",
      "Reward: 210.5\n",
      "Reward: 212.0\n",
      "Reward: 223.0\n",
      "Reward: 186.0\n",
      "Reward: 239.5\n",
      "Reward: 183.5\n",
      "Reward: 205.5\n",
      "Reward: 173.5\n",
      "Reward: 139.5\n",
      "Reward: 201.5\n",
      "Reward: 121.0\n",
      "Reward: 168.0\n",
      "Reward: 185.5\n",
      "Reward: 149.5\n",
      "Reward: 136.0\n",
      "Reward: 113.0\n",
      "Reward: 155.5\n",
      "Reward: 143.0\n",
      "Reward: 226.5\n",
      "Reward: 121.5\n",
      "Reward: 127.5\n",
      "Reward: 139.0\n",
      "Reward: 155.0\n",
      "Reward: 120.5\n",
      "Reward: 126.0\n",
      "Reward: 98.5\n",
      "Reward: 126.0\n",
      "Reward: 123.0\n",
      "Reward: 135.0\n",
      "Reward: 119.5\n",
      "Reward: 130.5\n",
      "Reward: 104.5\n",
      "Reward: 111.5\n",
      "Reward: 110.5\n",
      "Reward: 107.5\n",
      "Reward: 94.0\n",
      "Reward: 111.0\n",
      "Reward: 113.0\n",
      "Reward: 106.0\n",
      "Reward: 99.0\n",
      "Reward: 119.5\n",
      "Reward: 97.5\n",
      "Reward: 109.5\n",
      "Reward: 82.5\n",
      "Reward: 91.5\n",
      "Reward: 121.0\n",
      "Reward: 95.0\n",
      "Reward: 113.5\n",
      "Reward: 112.5\n",
      "Reward: 89.5\n",
      "Reward: 103.5\n",
      "Reward: 84.0\n",
      "Reward: 123.5\n",
      "Reward: 102.0\n",
      "Reward: 107.0\n",
      "Reward: 73.0\n",
      "Reward: 80.5\n",
      "Reward: 74.5\n",
      "Reward: 86.5\n",
      "Reward: 93.5\n",
      "Reward: 88.5\n",
      "Reward: 84.5\n",
      "Reward: 88.0\n",
      "Reward: 104.0\n",
      "Reward: 76.5\n",
      "Reward: 85.5\n",
      "Reward: 66.0\n",
      "Reward: 77.5\n",
      "Reward: 94.0\n",
      "Reward: 89.5\n",
      "Reward: 81.5\n",
      "Reward: 72.0\n",
      "Reward: 84.5\n",
      "Reward: 82.5\n",
      "Reward: 86.5\n",
      "Reward: 87.0\n",
      "Reward: 87.0\n",
      "Reward: 89.0\n",
      "Reward: 68.0\n",
      "Reward: 113.0\n",
      "Reward: 77.0\n",
      "Reward: 99.0\n",
      "Reward: 82.0\n",
      "Reward: 79.0\n",
      "Reward: 85.0\n",
      "Reward: 82.5\n",
      "Reward: 74.5\n",
      "Reward: 88.5\n",
      "Reward: 87.5\n",
      "Reward: 96.5\n",
      "Reward: 73.5\n",
      "WARNING:tensorflow:Error encountered when serializing data_augmentation.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef.\n",
      "'NoneType' object has no attribute 'name'\n",
      "WARNING:tensorflow:Error encountered when serializing data_preprocessing.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef.\n",
      "'NoneType' object has no attribute 'name'\n"
     ]
    }
   ],
   "source": [
    "num_episodes = 2000\n",
    "decay = 10\n",
    "max_time = 1200\n",
    "all_rewards = []\n",
    "saver = tf.train.Saver()\n",
    "train_data = []\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for i in range(num_episodes):\n",
    "        obs = env.reset()\n",
    "        obs = pixelNorm(obs)\n",
    "        episode_reward = 0\n",
    "        ep_history = []\n",
    "        for j in range(max_time+1):\n",
    "            #Choose an action\n",
    "            a_one_hot = sess.run(out, feed_dict={observation: [obs]}).reshape(env.action_space.n)\n",
    "            #print(a_one_hot)\n",
    "#             action = np.random.choice(a_one_hot, p=a_one_hot)\n",
    "#             action = np.argmax(a_one_hot == action)\n",
    "            if random.random() < (decay/(decay+i)):\n",
    "                action = np.random.choice(9)\n",
    "            else:\n",
    "                action = action = np.argmax(a_one_hot)\n",
    "            #print(action)\n",
    "            #env.render()\n",
    "            obs1, r, d, _ = env.step(action)\n",
    "            obs1 = pixelNorm(obs)\n",
    "            ep_history.append([obs, r, action])\n",
    "            obs = obs1\n",
    "            episode_reward += r\n",
    "            if d == True or j == max_time:\n",
    "                all_rewards.append(episode_reward)\n",
    "                ep_history = np.array(ep_history)\n",
    "                ep_history[:, 1] = discount_reward(ep_history[:, 1])\n",
    "                train_data.extend(ep_history)\n",
    "                if i % 1 == 0 and i != 0:\n",
    "                    train_data = np.array(train_data)\n",
    "                    \n",
    "                    shf = np.random.permutation(train_data[:, 1].shape[0])\n",
    "                    train_data[:, 0] = train_data[:, 0][shf]\n",
    "                    train_data[:, 1] = train_data[:, 1][shf]\n",
    "                    train_data[:, 2] = train_data[:, 2][shf]\n",
    "                    \n",
    "                    for prt in range(0, len(train_data), 16):\n",
    "                        _, q, q2 = sess.run([update, out, responsible_outputs], feed_dict={observation: np.vstack(train_data[prt:prt+16, 0]).reshape(-1, 210, 160, 3),\n",
    "                                                        reward_holder: train_data[prt:prt+16, 1],\n",
    "                                                        action_holder: train_data[prt:prt+16, 2]})\n",
    "                    train_data = []\n",
    "                break\n",
    "                \n",
    "        if i % 20 == 0 and i != 0:\n",
    "            print(\"Reward: \" + str(np.mean(all_rewards[-20:])))\n",
    "            \n",
    "    saver.save(sess, \"/tmp/model.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'seaborn' has no attribute 'plt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-61d0db81bd51>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mavg_reward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_rewards\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_rewards\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0msns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mavg_reward\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: module 'seaborn' has no attribute 'plt'"
     ]
    }
   ],
   "source": [
    "avg_reward = [np.mean(all_rewards[i-10:i+10]) for i in range(10, len(all_rewards))]\n",
    "sns.plt.plot(avg_reward[::10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /tmp/model.ckpt\n",
      "60.0\n",
      "60.0\n",
      "60.0\n",
      "60.0\n"
     ]
    }
   ],
   "source": [
    "max_time = 200\n",
    "saver = tf.train.Saver()\n",
    "    \n",
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, \"/tmp/model.ckpt\")\n",
    "    #Show the results\n",
    "    for i in range(10):\n",
    "        obs = env.reset()\n",
    "        episode_reward = 0\n",
    "        for j in range(max_time):\n",
    "            #Choose an action\n",
    "            a_one_hot = sess.run(out, feed_dict={observation: [obs]}).reshape(9)\n",
    "            action = np.random.choice(a_one_hot, p=a_one_hot)\n",
    "            action = np.argmax(a_one_hot == action)\n",
    "            env.render()\n",
    "            time.sleep(0.005)\n",
    "            obs, r, d, _ = env.step(action)\n",
    "            episode_reward += r\n",
    "            if d == True:\n",
    "                break\n",
    "        print(episode_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
