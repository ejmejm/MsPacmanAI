{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import time\n",
    "import pygame\n",
    "from sklearn.preprocessing import normalize\n",
    "import tensorflow as tf\n",
    "import tflearn\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"MsPacman-v0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Num actions: 9'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"Num actions: \" + str(env.action_space.n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Observation space: Box(210, 160, 3)'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"Observation space: \" + str(env.observation_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ejmejm/anaconda3/lib/python3.5/site-packages/tflearn/initializations.py:119: UniformUnitScaling.__init__ (from tensorflow.python.ops.init_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.initializers.variance_scaling instead with distribution=uniform to get equivalent behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ejmejm/anaconda3/lib/python3.5/site-packages/tensorflow/python/ops/gradients_impl.py:96: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    }
   ],
   "source": [
    "observation = tflearn.input_data(shape=[None, 210, 160, 3])\n",
    "net = tflearn.max_pool_2d(observation, 2)\n",
    "w1 = tflearn.conv_2d(observation, 32, 3, padding='valid', activation='relu')\n",
    "net = tflearn.max_pool_2d(w1, 2)\n",
    "net = tflearn.conv_2d(net, 32, 3, padding='valid', activation='relu')\n",
    "net = tflearn.max_pool_2d(net, 2)\n",
    "net = tflearn.conv_2d(net, 64, 3, padding='valid', activation='relu')\n",
    "net = tflearn.max_pool_2d(net, 2)\n",
    "net = tflearn.conv_2d(net, 64, 3, padding='valid', activation='relu')\n",
    "net = tflearn.max_pool_2d(net, 2)\n",
    "net = tflearn.conv_2d(net, 128, 3, padding='valid', activation='relu')\n",
    "net = tflearn.max_pool_2d(net, 2)\n",
    "net = tflearn.conv_2d(net, 128, 3, padding='valid', activation='relu')\n",
    "net = tflearn.fully_connected(net, 1028, activation='relu')\n",
    "out = tflearn.fully_connected(net, env.action_space.n, activation=\"softmax\")\n",
    "\n",
    "reward_holder = tf.placeholder(tf.float32, [None])\n",
    "action_holder = tf.placeholder(tf.int32, [None])\n",
    "\n",
    "responsible_outputs = tf.gather(tf.reshape(out, [-1]), tf.range(0, tf.shape(out)[0] * tf.shape(out)[1], env.action_space.n) + action_holder)\n",
    "\n",
    "loss = -tf.reduce_mean(tf.log(responsible_outputs + 0.0001) * reward_holder)\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=0.0005)\n",
    "update = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = 0.99\n",
    "\n",
    "def discount_reward(rewards):\n",
    "    running_reward = 0\n",
    "    result = np.zeros_like(rewards)\n",
    "    for i in reversed(range(len(rewards))):\n",
    "        result[i] = rewards[i] + gamma * running_reward\n",
    "        running_reward += rewards[i]\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pixelNorm(img):\n",
    "    return img / 255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reward: 290.5\n",
      "Reward: 367.0\n",
      "Reward: 357.5\n",
      "Reward: 364.0\n",
      "Reward: 238.5\n",
      "Reward: 235.0\n",
      "Reward: 230.0\n"
     ]
    }
   ],
   "source": [
    "num_episodes = 2000\n",
    "decay = 10\n",
    "max_time = 1200\n",
    "all_rewards = []\n",
    "saver = tf.train.Saver()\n",
    "train_data = []\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for i in range(num_episodes):\n",
    "        obs = env.reset()\n",
    "        obs = pixelNorm(obs)\n",
    "        episode_reward = 0\n",
    "        ep_history = []\n",
    "        for j in range(max_time+1):\n",
    "            #Choose an action\n",
    "            a_one_hot = sess.run(out, feed_dict={observation: [obs]}).reshape(env.action_space.n)\n",
    "            #print(a_one_hot)\n",
    "#             action = np.random.choice(a_one_hot, p=a_one_hot)\n",
    "#             action = np.argmax(a_one_hot == action)\n",
    "            if random.random() < (decay/(decay+i)):\n",
    "                action = np.random.choice(9)\n",
    "            else:\n",
    "                action = action = np.argmax(a_one_hot)\n",
    "            #print(action)\n",
    "            #env.render()\n",
    "            obs1, r, d, _ = env.step(action)\n",
    "            obs1 = pixelNorm(obs)\n",
    "            ep_history.append([obs, r, action])\n",
    "            obs = obs1\n",
    "            episode_reward += r\n",
    "            if d == True or j == max_time:\n",
    "                all_rewards.append(episode_reward)\n",
    "                ep_history = np.array(ep_history)\n",
    "                ep_history[:, 1] = discount_reward(ep_history[:, 1])\n",
    "                train_data.extend(ep_history)\n",
    "                if i % 1 == 0 and i != 0:\n",
    "                    train_data = np.array(train_data)\n",
    "                    \n",
    "                    shf = np.random.permutation(train_data[:, 1].shape[0])\n",
    "                    train_data[:, 0] = train_data[:, 0][shf]\n",
    "                    train_data[:, 1] = train_data[:, 1][shf]\n",
    "                    train_data[:, 2] = train_data[:, 2][shf]\n",
    "                    \n",
    "                    for prt in range(0, len(train_data), 16):\n",
    "                        _, q, q2 = sess.run([update, out, responsible_outputs], feed_dict={observation: np.vstack(train_data[prt:prt+16, 0]).reshape(-1, 210, 160, 3),\n",
    "                                                        reward_holder: train_data[prt:prt+16, 1],\n",
    "                                                        action_holder: train_data[prt:prt+16, 2]})\n",
    "                    train_data = []\n",
    "                break\n",
    "                \n",
    "        if i % 20 == 0 and i != 0:\n",
    "            print(\"Reward: \" + str(np.mean(all_rewards[-20:])))\n",
    "            \n",
    "    saver.save(sess, \"/tmp/model.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_reward = [np.mean(all_rewards[i-10:i+10]) for i in range(10, len(all_rewards))]\n",
    "sns.plt.plot(avg_reward[::10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_time = 200\n",
    "saver = tf.train.Saver()\n",
    "    \n",
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, \"/tmp/model.ckpt\")\n",
    "    #Show the results\n",
    "    for i in range(10):\n",
    "        obs = env.reset()\n",
    "        obs = obs.reshape([-1])\n",
    "        episode_reward = 0\n",
    "        for j in range(max_time):\n",
    "            #Choose an action\n",
    "            a_one_hot = sess.run(out, feed_dict={observation: [obs]}).reshape(9)\n",
    "            action = np.random.choice(a_one_hot, p=a_one_hot)\n",
    "            action = np.argmax(a_one_hot == action)\n",
    "            env.render()\n",
    "            time.sleep(0.005)\n",
    "            obs, r, d, _ = env.step(action)\n",
    "            obs = obs.reshape([-1])\n",
    "            episode_reward += r\n",
    "            if d == True:\n",
    "                break\n",
    "        print(episode_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
